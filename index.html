<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | CS, Georgia Tech | Fall 2019: CS 6476</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Nothin' But Nets</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Bryan Baek, Christopher Banks, Clay Dodson, Dasom Eom</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2019 CS 6476: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>
<!-- Goal -->
<h3>Problem Statement</h3>
In the physical world, humans draw and write their abstract idea or knowledge to better explain to audiences. As computer technology has developed, streaming media emerges as new visual means to convey information to distant audiences. Although humans can easily understand the concept displayed in streaming video, machines do not recognize the contents in the images embedded in video, and thus the information in the streaming video cannot be processed to be used in the computer system. At this moment, we propose to train a Convolutional Neural Network (CNN) to classify single/multiple handwritten digits from live video feed so that the handwritten digits displayed in the live video can be converted into machine-encoded text. 
  <br><br>
<!-- figure -->
<h3>Approach</h3>
First, we will use OpenCV to process the live feed and send the frames of images to the backend Python file, and to display the classification on the video feed. We seek to have an app that displays the captured video feed from the default camera/webcam, and overlays the predicted bounding boxes and digits on the screen in real time. Then, we plan to train a CNN using PyTorch to perform the end-to-end task of simultaneously recognizing and localizing all the digits of a number. 
<br><br>
<!-- Main Illustrative Figure --> 
<div style="text-align: center;">
<!--<img style="height: 200px;" alt="" src="mainfig.png"> -->
</div>

<br><br>
<!-- Experiments -->
<h3>Experiments</h3>
We will be using the MNIST Dataset and the SVHN Dataset to train a Convolutional Neural Network (CNN). These datasets contain thousands of handwritten digits and digits recorded from video data. Our experimental procedure can be broken into three steps:
  <ul>  
  <li>Train a CNN on the MNIST dataset to classify single/multiple handwritten digits. This can be a way to quickly test out the model architecture and gives us a baseline performance of the model.  </li>
  <li>Train a CNN on the SVHN dataset to classify single/multiple handwritten digits</li>
  <li>Evaluate the efficacy of these CNNs to classify single/multiple handwritten digits from live video data.</li>
  <li>We also seek to compare the performances depending on different architectures/dataset. We will evaluate per digit and whole number accuracy and precision.</li>
  </ul>
  <br><br>
<!-- Results -->
<h3>Results</h3>
When we shoot video and introduce the live video feed to the system, the system will process the image frames and extract only the digits, and the digits on the video screen will be marked with green border around, and the equivalent digits will be printed out.
<br><br>
  
  <hr>
  <footer> 
  <p>Â© Bryan Baek, Christopher Banks, Clay Dodson, Dasom Eom</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
